# Python-for-DS-project
In this Project, I have analyzed a real-world dataset on housing sale prices and predicted the final price of each home using data exploration, data visualization, and regression techniques.

In module 1, I started off with exploring the training and test data sets and found that each had 81 columns with 1200 and 260 rows, respectively. I dropped the 'Id' column since it was useless. I then observed the skewness of the target variable (Sale Price) and found that it was skewed beyond the acceptable range. Therefore, I fixed the skew using log transformation. Further going on, I concatenated the test and training datasets, so that data processing is faster. Later, I observed the features with high percentage of missing values and dropped them since they were not very useful and could offer less insight. For the rest of the missing values, I imputed them with appropriate substituted values. Finally, I checked our numerical variables for skewness and fixed those which had very high skew values using log transformation. I had initially performed a conventional outlier removal method, but did not go ahead with it since it caused a significant loss of data. My motivation behind using log transformation was to retain the data.

In the end, we were left with 74 features, including the target variable, and the same number of observations/rows/instances as the original dataset.

In module 2, I split the data to obtain our training dataset for visualization purposes. Firstly, I visualized the correlation of numerical variables with the target variable, Sale Price, to observe and distinguish which ones had strong and weak associations. Amomng those features which had strong correlation with Sale Price, OverallQual, GrLivArea, GarageCars, GarageArea, X1stFlrSF, FullBath, YearBuilt, YearRemodAdd, TotRmsAbvGrd, and Fireplaces were included. Secondly, I plotted heatmaps to observe the correlation of variables with each other. For two variables with strong correlation with each other, I dropped one of them as I wanted to avoid the multicollinearity conundrum, which would otherwise have produced model results that are not very reliable. For example, GarageCars and GarageArea had a very high correlation coefficient (0.88) and I had to remove one of them. I ended up dropping GarageArea because it had a lower correlation coefficient with the target variable than GarageCars.

Thereafter, we were left with the following numerical features:

1) OverallQual 2) GrLivArea 3) GarageCars 4) TotalBsmtSF 5) FullBath 6) YearBuilt
As for the other visualizations I produced, it can be categorized into two parts. Firstly, I made scatterplots for the top features that were highly correlated with Sale Price. The scatterplots helped gain promising insights and made the feature removal process easier by visualizing the correlations. Secondly, I made violin plots for all the categorical variables with respect to Sale Price. I used violin plots as they will help show the range and frequency of the data (through the width of the violin). Most features were more frequently distributed in the mid-price range (250000 - 260000).

In module 3, in order to accomplish dimensional reduction, I first scaled the data. Then, I calculated the ideal number of clusters based on the assumption that inertia should be minimal. I decided on 7 clusters because the graph indicates that once the number of clusters reaches 7 or more, the rate of inertia fall stabilizes.

I attempted to visualize the clusters using PCA in an effort to minimize the number of dimensions in the data to 2. Next, I worked to determine the optimal number of components to account for 95% of the variance among the components.

I determined that 20 components will account for about 95% of the variance. Comparing PCA with 20 components to PCA with 2 components previously employed, we can observe that inertia decreased and rate of decrease was also stable.

In module 4, I chose Lasso, Ridge, and Support Vector Regression methods and built models using 5-fold cross-validation. I used the metric RMSE to evaluate the results. RMSE is one of the most popular measures to estimate the accuracy of our forecasting modelâ€™s predicted values versus the actual values. The RMSE values of each model were fairly good and did not vary a lot from each other other as could be seen through the visualization. The lower values of RMSE indicate a better fit.

Lastly, I visualized the importance of features for two models, Lasso and Ridge. Note that there are many coinciding important features between the two models. All the elements in green have a positive impact on the sale price of a home, i.e. increases the price. Conversely, the red elements have a negative impact on a house's sale price.
